{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kolmogorov-Smirnov test    \n",
    "by Tonatiuh Rangel\n",
    "\n",
    "## Contents:    \n",
    "1. [Theory](#theory)   \n",
    "2. [Numerical example](#example)\n",
    "3. [Scipy's example](#scipy)\n",
    "\n",
    "\n",
    "<a id='theory'></a>\n",
    "Also called \"Kolmogorov-Smirnov goodness of fit test\".   \n",
    "This is used to test if two samples are drawn from different distributions.    \n",
    "\n",
    "\n",
    "### Hypothesis    \n",
    "**Null hypothesis**: the two samples come from the same distribution    \n",
    "**Alternative hypothesis**: the two samples are drawn from different distributions      \n",
    "\n",
    "### Test statistic    \n",
    "The test statistic is computed as the maximum distance between the corresponding two cumulative distribution functions (CDF) for the two samples.    \n",
    "\n",
    "### P-value    \n",
    "P-value is obtained from tables for the two-sided KS test. \n",
    "Here, I use the scikit-learn's cumulative distribution function of Kolmogorov's distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.figsize\": (6, 4)})\n",
    "sns.set_context('talk')\n",
    "\n",
    "from scipy.stats import ks_2samp, kstwobign \n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cdfs(X, Y):\n",
    "    \"\"\"\n",
    "    Get CDF functions for X, Y\n",
    "    \n",
    "    Arguments\n",
    "    X, numpy array\n",
    "    Y, numpy array\n",
    "    \n",
    "    Returns\n",
    "    CDF_X(x), function \n",
    "    CDF_Y(x), function\n",
    "    \"\"\"\n",
    "    n1 = len(X)\n",
    "    n2 = len(Y)\n",
    "    data1 = np.sort(X)\n",
    "    data2 = np.sort(Y)\n",
    "    data_all = np.concatenate([data1,data2])\n",
    "    cdf1 = np.searchsorted(data1,data_all,side='right')/(1.0*n1)\n",
    "    cdf2 = (np.searchsorted(data2,data_all,side='right'))/(1.0*n2)\n",
    " \n",
    "    # Now that we have computed CDF_X and CDF_Y for discrete set of values, we need to make them continuous functions\n",
    "    def CDF_X(x):\n",
    "        ff = interp1d(data_all, cdf1, kind='slinear')\n",
    "        return ff(x)\n",
    "    def CDF_Y(x):\n",
    "        ff = interp1d(data_all, cdf2, kind='slinear')\n",
    "        return ff(x) \n",
    "    return CDF_X, CDF_Y\n",
    "\n",
    "\n",
    "def apply_kolmogorov_test(n1, n2, CDF1, CDF2):\n",
    "    \n",
    "    # Evaluate CDF functions for a discrete number of samples and get maximum diff:   \n",
    "    x = np.linspace(max(np.min(X), np.min(Y)), min(np.max(X), np.max(Y)), 10000)\n",
    "    \n",
    "    # Get the maximum distance between the two distributions:\n",
    "    diff = np.abs(CDF1(x) - CDF2(x))\n",
    "    KS_statistic = diff.max()\n",
    "    # Get the index for the max. between the two distributions, for plotting\n",
    "    KS_index = x[diff.argmax()]\n",
    "    \n",
    "    # Pvalue from https://github.com/scipy/scipy/blob/v0.19.0/scipy/stats/stats.py\n",
    "    try:\n",
    "        pvalue = distributions.kstwobign.sf((en + 0.12 + 0.11 / en) * KS_statistic)\n",
    "    except:\n",
    "        pvalue = 1.0\n",
    "    \n",
    "    return KS_statistic, pvalue,  KS_index\n",
    "\n",
    "def plot_distributions(X, Y):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    sns.distplot(X, label='X', ax=ax)\n",
    "    sns.distplot(Y, label='Y', ax=ax)\n",
    "    ax.set_xlabel('Sepal length (cm)')\n",
    "    ax.legend(loc=0)\n",
    "    sns.despine()\n",
    "\n",
    "def plot_kolmogorov_test(x, CDF_X, CDF_Y, KS_index):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(x, CDF_X(x), color='blue', linestyle='-', lw=5, alpha=0.6, label = 'X' )\n",
    "    ax.plot(x, CDF_Y(x), color='orange', linestyle='-', lw=5, alpha=0.6, label = 'Y')\n",
    "\n",
    "    ax.plot([KS_index, KS_index], [CDF_X(KS_index), CDF_Y(KS_index)], color='k', linestyle='-', linewidth=2)\n",
    "    \n",
    "    ax.text(KS_index + np.max(x) / 200 , 0.5, 'KS statistic',color='k')\n",
    "    ax.legend(loc=0)\n",
    "    ax.set_ylabel('CDF')\n",
    "    ax.set_xlabel('Sepal length (cm)')\n",
    "\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='example'></a>    \n",
    "## Numerical example    \n",
    "Let's use the KS-test to tell whether two samples come from the same distribution.    \n",
    "In this example, I test data from the *iris dataset*, which contains samples for different iris species.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']], columns= iris['feature_names'] + ['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples of sepal lenth for two different species\n",
    "X = df[df.target == 0]['sepal length (cm)'].values\n",
    "Y = df[df.target == 1]['sepal length (cm)'].values\n",
    "\n",
    "# Its a good idea to remove outliers\n",
    "#X = X[np.where((X > np.percentile(X, 1)) & (X < np.percentile(X, 99)))]\n",
    "#Y = Y[np.where((Y > np.percentile(Y, 1)) & (Y < np.percentile(Y, 99)))]\n",
    "\n",
    "print('Sample X of size {} and sample Y of size {}'.format(len(X), len(Y)))\n",
    "# Lets visualize the sample distributions\n",
    "plot_distributions(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute KS statistic and p-value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I get cumulative distribution functions\n",
    "CDF_X, CDF_Y = get_cdfs(X, Y)\n",
    "\n",
    "# Now find the maximum difference between the two CDF\n",
    "KS_statistic, pvalue, KS_index = apply_kolmogorov_test(len(X), len(Y), CDF_X, CDF_Y)\n",
    "print('KS_statistic {}, p-value {}'.format(KS_statistic, pvalue))\n",
    "\n",
    "# Plot the CDFs and KS statistic\n",
    "# Evaluate CDF functions for a discrete number of samples  \n",
    "x = np.linspace(max(np.min(X), np.min(Y)), min(np.max(X), np.max(Y)), 10000)\n",
    "\n",
    "plot_kolmogorov_test(x, CDF_X, CDF_Y, KS_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see the two CDFs and the KS statistic corresponding to the maximum distance between the two CDFs.    \n",
    "The p-value for the KS statistic of 0.78 is virtually zero, hence we reject the null hypothesis that both samples are drawn from the same distribution.\n",
    "\n",
    "\n",
    "<a id='scipy'></a>   \n",
    "## Scipy    \n",
    "Now let's repeat the test using the scipy's implementation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
